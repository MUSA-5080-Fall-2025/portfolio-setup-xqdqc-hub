---
title: "QianmuZheng_Assignment4"
output: html_document
date: "2025-11-10"
---
## Step 1: Data Loading & Preparation
Graffiti is one of the most common urban maintenance issues in Chicago.
In this step, I downloaded the 311 Graffiti Removal service request data from the City of Chicago Data Portal.

```{r}
# Step 1: Load and Prepare Data

library(tidyverse)
library(sf)
library(jsonlite)
library(lubridate)

# Load Graffiti Removal 311 data from Chicago Open Data API
url <- "https://data.cityofchicago.org/resource/hec5-y4x5.json?$limit=50000"

data_raw <- fromJSON(url)

```

```{r}
# Select and clean relevant columns
data_clean <- data_raw %>%
  select(service_request_number,
         creation_date,
         completion_date,
         status,
         street_address,
         latitude,
         longitude,
         ward,
         community_area) %>%
  mutate(
    creation_date = ymd_hms(creation_date),
    completion_date = ymd_hms(completion_date),
    year = year(creation_date),
    month = month(creation_date, label = TRUE),
    day = day(creation_date)
  ) %>%
  drop_na(latitude, longitude)

```

```{r}
# Convert to spatial (sf object)
data_sf <- st_as_sf(
  data_clean,
  coords = c("longitude", "latitude"),
  crs = 4326
)
```

```{r}
library(ggplot2)
library(sf)
library(dplyr)

ggplot() +
  geom_sf(data = data_sf, aes(), 
          color = "#e41a1c", alpha = 0.4, size = 0.6) +

  labs(
    title = "Spatial Distribution of Street Light Out Reports (Chicago)",
    subtitle = "Each point represents a 311 service request for streetlight outage",
    caption = "Source: City of Chicago Open Data Portal"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 15, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 9, color = "gray40"),
    panel.background = element_rect(fill = "aliceblue")
  )

```

## Step 2: Fishnet Grid Creation
In this step, I created a 500-meter fishnet grid across the City of Chicago to aggregate the Graffiti Removal 311 requests. By counting how many graffiti reports fall into each grid cell, we can visualize where graffiti problems are concentrated and prepare the data for spatial regression analysis.

```{r}
data_proj <- st_transform(data_sf, crs = 26971)

bbox <- st_bbox(data_proj)

fishnet <- st_make_grid(
  st_as_sfc(bbox),
  cellsize = 500,
  square = TRUE
) %>%
  st_sf() %>%
  mutate(grid_id = row_number())
```

```{r}
# Visualize the fishnet grid outline
ggplot() +
  geom_sf(data = fishnet, fill = NA, color = "grey40", size = 0.2) +
  geom_sf(data = data_sf, color = "red", size = 0.5, alpha = 0.6) +
  labs(
    title = "500m Fishnet Grid over Chicago Graffiti Reports",
    subtitle = "Each square represents a 500m x 500m cell",
    caption = "Data source: Chicago Open Data Portal"
  ) +
  theme_minimal()
```


```{r}
st_crs(fishnet) <- st_crs(data_proj)

fishnet_counts <- st_join(fishnet, data_proj, join = st_contains)

# Count number of graffiti incidents per grid cell
fishnet_summary <- fishnet_counts %>%
  st_drop_geometry() %>%
  group_by(grid_id) %>%
  summarise(graffiti_count = n()) %>%
  right_join(fishnet, by = "grid_id") %>%
  st_as_sf()
```

```{r}
ggplot() +
  geom_sf(data = fishnet_summary, aes(fill = graffiti_count), color = "grey20") +
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  labs(
    title = "500m Fishnet Grid over Chicago Graffiti Reports",
    subtitle = "Graffiti points (red) aggregated within each grid cell",
    fill = "Graffiti Count"
  ) +
  theme_minimal()
```


```{r}
library(spdep)

data_sf_grid <- fishnet_summary %>%
  filter(!is.na(graffiti_count)) %>%
  mutate(id = row_number())
```
```{r}
nb <- poly2nb(data_sf_grid)
lw <- nb2listw(nb, style = "W", zero.policy = TRUE)

local_moran <- localmoran(
  x = data_sf_grid$graffiti_count,
  listw = lw,
  zero.policy = TRUE
)
```

```{r}
data_sf_grid <- data_sf_grid %>%
  mutate(
    local_I = local_moran[, 1],
    local_p = local_moran[, 5],
    hot_cold = case_when(
      local_I > 0 & local_p < 0.05 ~ "Hot Spot",
      local_I < 0 & local_p < 0.05 ~ "Cold Spot",
      TRUE ~ "Not Significant"
    )
  )
```

```{r}
ggplot(data_sf_grid) +
  geom_sf(aes(fill = hot_cold), color = NA) +
  scale_fill_manual(
    values = c("Hot Spot" = "#d7191c", "Cold Spot" = "#2c7bb6", "Not Significant" = "grey80")
  ) +
  labs(
    title = "Local Moran’s I: Graffiti Removal Hot & Cold Spots",
    subtitle = "City of Chicago, 2024",
    fill = "Cluster Type"
  ) +
  theme_minimal()
```
## Step 4: Count Regression Models

In this step, I built statistical models to predict the number of graffiti removal requests per grid cell, and applied Poisson regression and Negative Binomial regression, which are designed for modeling count data. By comparing model fit using the Akaike Information Criterion (AIC), we can determine which model better captures the spatial variability of graffiti incidents.

```{r}
library(MASS)
library(broom)

data_sf_grid$neighbor_mean <- lag.listw(lw, data_sf_grid$graffiti_count, zero.policy = TRUE)

model_poisson <- glm(
  graffiti_count ~ neighbor_mean + local_I,
  data = data_sf_grid,
  family = "poisson"
)

summary(model_poisson)
```
```{r}
model_nb <- glm.nb(
  graffiti_count ~ neighbor_mean + local_I,
  data = data_sf_grid
)
```

```{r}
aic_comparison <- data.frame(
  Model = c("Poisson", "Negative Binomial"),
  AIC = c(AIC(model_poisson), AIC(model_nb))
)
```

```{r}
poisson_summary <- tidy(model_poisson)
nb_summary <- tidy(model_nb)


print(poisson_summary)
print(nb_summary)
```
```{r}
data_sf_grid <- data_sf_grid %>%
  mutate(
    pred_poisson = predict(model_poisson, type = "response"),
    pred_nb = predict(model_nb, type = "response")
  )

ggplot(data_sf_grid, aes(x = graffiti_count, y = pred_nb)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(
    title = "Observed vs Predicted (Negative Binomial Model)",
    x = "Observed Graffiti Count",
    y = "Predicted Count"
  ) +
  theme_minimal()
```
## Step 5: Spatial Cross-Validation

To assess how well the model generalizes across space, I performed spatial cross-validation. Using a LOGO approach, I trained the model on all grid cells except one spatial group and tested performance on the left-out group.

```{r}
library(Metrics)

set.seed(42)
data_sf_grid$group_id <- kmeans(st_coordinates(st_centroid(data_sf_grid)), centers = 10)$cluster

cv_results <- data.frame(group_id = character(), MAE = numeric(), RMSE = numeric())

for (g in unique(data_sf_grid$group_id)) {

  train_data <- data_sf_grid[data_sf_grid$group_id != g, ]
  test_data  <- data_sf_grid[data_sf_grid$group_id == g, ]

  model_cv <- glm(
    graffiti_count ~ neighbor_mean + local_I,
    data = train_data,
    family = "poisson"
  )
  
  # Predict on test data
  test_data$pred <- predict(model_cv, newdata = test_data, type = "response")
  
  # Compute errors
  mae_val <- mae(test_data$graffiti_count, test_data$pred)
  rmse_val <- rmse(test_data$graffiti_count, test_data$pred)
  
  # Store results
  cv_results <- rbind(cv_results, data.frame(group_id = g, MAE = mae_val, RMSE = rmse_val))
}
```

```{r}
cv_summary <- cv_results %>%
  summarise(
    Mean_MAE = mean(MAE),
    Mean_RMSE = mean(RMSE)
  )

print(cv_summary)
```
```{r}
ggplot(cv_results, aes(x = as.factor(group_id), y = RMSE)) +
  geom_bar(stat = "identity", fill = "#3182bd") +
  labs(
    title = "Spatial Cross-Validation (LOGO) RMSE by Group",
    x = "Spatial Group (Community Area)",
    y = "RMSE"
  ) +
  theme_minimal()
```
## Step 6: Model Visulization
In this step, I evaluated the spatial performance of the Poisson regression model by comparing predicted graffiti counts with the observed counts across the city.

First, I generated predicted graffiti counts for each 500m grid cell and computed residuals. Then, I visualized the spatial distribution of these residuals to identify over- and under-predicted areas. Additionally, I compared the regression-based predictions with a KDE map as a baseline.

```{r}
# Generate predictions and residuals
data_sf_grid <- data_sf_grid %>%
  mutate(
    predicted = predict(model_poisson, type = "response"),
    residual = graffiti_count - predicted
  )

# Map observed vs predicted counts
ggplot(data_sf_grid) +
  geom_sf(aes(fill = predicted), color = NA) +
  scale_fill_viridis_c(option = "magma", trans = "sqrt") +
  labs(
    title = "Predicted Graffiti Counts (Poisson Model)",
    subtitle = "City of Chicago, 500m grid",
    fill = "Predicted Count"
  ) +
  theme_minimal()
```
```{r}
# Map residuals (model error)
ggplot(data_sf_grid) +
  geom_sf(aes(fill = residual), color = NA) +
  scale_fill_gradient2(
    low = "#4575b4", mid = "white", high = "#d73027",
    midpoint = 0
  ) +
  labs(
    title = "Model Residuals (Observed - Predicted)",
    subtitle = "Blue = under-predicted, Red = over-predicted",
    fill = "Residual"
  ) +
  theme_minimal()
```


```{r}
# KDE baseline comparison (fixed version)
library(spatstat)
library(stars)

# Transform to projected CRS (in meters, for distance-based KDE)
data_sf_proj <- st_transform(data_sf, 26971)  # NAD83 / Illinois East (meters)

# Convert to spatstat ppp object
coords <- st_coordinates(data_sf_proj)

# Create observation window from study area (fishnet extent)
win <- as.owin(st_union(st_geometry(st_transform(data_sf_grid, 26971))))

# Create ppp point pattern
graffiti_ppp <- ppp(
  x = coords[, 1],
  y = coords[, 2],
  window = win
)

# Compute KDE
graffiti_kde <- density.ppp(graffiti_ppp, sigma = 500)  # bandwidth = 500m

# Convert to data frame for ggplot
kde_df <- as.data.frame(graffiti_kde)
colnames(kde_df) <- c("x", "y", "density")

```
```{r}
ggplot(kde_df, aes(x, y, fill = density)) +
  geom_raster() +
  coord_equal() +
  scale_fill_viridis_c(option = "inferno") +
  labs(
    title = "KDE Baseline – Graffiti Removal Density",
    subtitle = "Kernel Density of 311 Graffiti Removal Reports (σ = 500m)",
    fill = "Density"
  ) +
  theme_void()
```
## Step 7: Conclusion

The results revealed clear spatial clustering patterns — high graffiti concentrations were found in central neighborhoods, while peripheral areas showed lower activity. The Negative Binomial model provided a better fit than the Poisson model, suggesting the presence of overdispersion in the data. Spatial cross-validation further confirmed that the model’s predictive accuracy varies across space, performing well in dense urban areas but less accurately in outlying regions.

However, the residual analysis showed some underestimation in highly concentrated graffiti areas, indicating that incorporating additional contextual variables, such as land use, population density, or socioeconomic characteristics, could further enhance model accuracy.

